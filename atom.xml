<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coisini</title>
  
  <subtitle>一見旧知のようです、万千の歓喜心が生まれます</subtitle>
  <link href="https://www.enju-tsubaki.icu/atom.xml" rel="self"/>
  
  <link href="https://www.enju-tsubaki.icu/"/>
  <updated>2025-02-10T04:55:02.199Z</updated>
  <id>https://www.enju-tsubaki.icu/</id>
  
  <author>
    <name>Coisini</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.enju-tsubaki.icu/posts/3610a686.html"/>
    <id>https://www.enju-tsubaki.icu/posts/3610a686.html</id>
    <published>2025-02-15T06:02:27.153Z</published>
    <updated>2025-02-10T04:55:02.199Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>🌟 <strong>简单的自我介绍</strong></p><p>大家好，我是 Coisini，来自古城西安，是陕西理工大学数计学院人工智能专业的学生。</p><p>🚀 <strong>编程的目标方向</strong></p><p>我对编程充满热情，尤其专注于人工智能领域的研究与应用，致力于探索机器学习和深度学习算法的潜力。通过结合Spring Boot框架，我能够快速构建稳定的应用程序，为AI算法的实际部署提供强有力的支持。利用MyBatis简化数据库操作，使我更专注于业务逻辑优化和算法实现，而Maven则帮助我高效管理项目依赖和构建过程，使开发更加流畅。每次攻克技术难题、掌握新概念，都让我感到极大的满足与成就，力求在AI技术的研究与实践中不断前进。</p><p>🌱 <strong>未来的学习方向</strong></p><p>在这个快速发展的时代，我立志于紧跟人工智能领域的前沿趋势，深入学习并掌握一系列关键技术，包括但不限于DeepSeek、Cursor、Dify、工作流、智能体以及知识库等。这些技术代表了当前AI技术发展的重要方向，它们不仅能够提升我的专业技能，也为解决复杂问题提供了新的思路和工具。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="个人简介" scheme="https://www.enju-tsubaki.icu/categories/%E4%B8%AA%E4%BA%BA%E7%AE%80%E4%BB%8B/"/>
    
    <category term="编程学习" scheme="https://www.enju-tsubaki.icu/categories/%E4%B8%AA%E4%BA%BA%E7%AE%80%E4%BB%8B/%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://www.enju-tsubaki.icu/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习" scheme="https://www.enju-tsubaki.icu/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://www.enju-tsubaki.icu/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="SpringBoot" scheme="https://www.enju-tsubaki.icu/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>K-近邻算法API</title>
    <link href="https://www.enju-tsubaki.icu/posts/d5f3c9dd.html"/>
    <id>https://www.enju-tsubaki.icu/posts/d5f3c9dd.html</id>
    <published>2025-02-14T05:51:33.482Z</published>
    <updated>2025-02-14T06:31:45.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="🌟1-2-k近邻算法api初步使用🌟"><a href="#🌟1-2-k近邻算法api初步使用🌟" class="headerlink" title="🌟1.2 k近邻算法api初步使用🌟"></a>🌟1.2 k近邻算法api初步使用🌟</h1><h2 id="🌈学习目标"><a href="#🌈学习目标" class="headerlink" title="🌈学习目标"></a>🌈学习目标</h2><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul><li>了解sklearn工具的优点和包含内容</li><li>应用sklearn中的api实现KNN算法的简单使用</li></ul><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="Alt text"></p><ol><li>获取数据集</li><li>数据基本处理</li><li>特征工程</li><li>机器学习</li><li>模型评估</li></ol><h2 id="🛠️Scikit-learn-工具介绍"><a href="#🛠️Scikit-learn-工具介绍" class="headerlink" title="🛠️Scikit-learn 工具介绍"></a>🛠️Scikit-learn 工具介绍</h2><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/scikitlearn.png" alt="Alt text"></p><p>Scikit-learn是Python语言的机器学习工具，它具有以下特点：</p><ul><li>包括许多知名的机器学习算法的实现</li><li>文档完善，容易上手，拥有丰富的AP</li></ul><h3 id="🔧安装"><a href="#🔧安装" class="headerlink" title="🔧安装"></a>🔧安装</h3><p>使用以下命令进行安装：<br><code>pip3 install scikit-learn</code></p><p>安装好之后可以通过以下命令查看是否安装成功：<br><code>import sklearn</code></p><p>注：安装scikit-learn需要Numpy, Scipy等库。</p><h3 id="📦Scikit-learn包含的内容"><a href="#📦Scikit-learn包含的内容" class="headerlink" title="📦Scikit-learn包含的内容"></a>📦Scikit-learn包含的内容</h3><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/sklearn%E5%8C%85%E5%90%AB%E5%86%85%E5%AE%B9.png" alt="Alt text"></p><ul><li>分类、聚类、回归 - 特征工程</li><li>模型选择、调优</li></ul><h2 id="📌K-近邻算法API"><a href="#📌K-近邻算法API" class="headerlink" title="📌K-近邻算法API"></a>📌K-近邻算法API</h2><p><code>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)</code></p><ul><li><code>n_neighbors</code>：int，可选（默认 = 5），k_neighbors 查询默认使用的邻居数</li></ul><h2 id="📋案例"><a href="#📋案例" class="headerlink" title="📋案例"></a>📋案例</h2><h3 id="👣步骤分析"><a href="#👣步骤分析" class="headerlink" title="👣步骤分析"></a>👣步骤分析</h3><ol><li>获取数据集</li><li>数据基本处理（该案例中省略）</li><li>特征工程（该案例中省略）</li><li>机器学习</li><li>模型评估（该案例中省略）</li></ol><h3 id="💻代码过程"><a href="#💻代码过程" class="headerlink" title="💻代码过程"></a>💻代码过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line">x = [[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 机器学习 -- 模型训练</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">result = estimator.predict([[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h2 id="❓问题解答"><a href="#❓问题解答" class="headerlink" title="❓问题解答"></a>❓问题解答</h2><h3 id="1-选取K值的大小？"><a href="#1-选取K值的大小？" class="headerlink" title="1. 选取K值的大小？"></a>1. 选取K值的大小？</h3><p>K值的选择对KNN模型有重要影响：</p><ul><li><strong>K值过小：</strong><ul><li>容易受到异常点的影响</li><li>容易过拟合，即“学习”近似误差会减小，但“学习”的估计误差会增大，整体模型变得复杂。</li></ul></li><li><strong>K值过大：</strong><ul><li>受到样本均衡的问题</li><li>容易欠拟合，学习的近似误差会增大，模型变得简单。</li></ul></li><li><strong>K=N（N为训练样本个数）：</strong>完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。</li></ul><p>在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是把训练数据再分成两组：训练集和验证集）来选择最优的K值。</p><p><strong>近似误差：</strong>对现有训练集的训练误差，关注训练集。如果近似误差过小可能会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测，模型本身不是最接近最佳模型。</p><p><strong>估计误差：</strong>可以理解为对测试集的测试误差，关注测试集。估计误差小说明对未知数据的预测能力好，模型本身最接近最佳模型。</p><h3 id="2-api中其他参数的具体含义？"><a href="#2-api中其他参数的具体含义？" class="headerlink" title="2. api中其他参数的具体含义？"></a>2. api中其他参数的具体含义？</h3><p><code>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm=&#39;auto&#39;)</code></p><ul><li><code>n_neighbors</code>：int，可选（默认 = 5），k_neighbors 查询默认使用的邻居数</li><li><code>algorithm</code>：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，快速 k 近邻搜索算法，默认参数为 auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法：<ul><li><code>brute</code>：蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。</li><li><code>kd_tree</code>：构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。</li><li><code>ball_tree</code>：是为了克服kd树高维失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。</li></ul></li></ul><h2 id="📚总结"><a href="#📚总结" class="headerlink" title="📚总结"></a>📚总结</h2><h3 id="sklearn的优势"><a href="#sklearn的优势" class="headerlink" title="sklearn的优势"></a>sklearn的优势</h3><ul><li>文档多，且规范</li><li>包含的算法多 - 实现起来容易</li></ul><h3 id="knn中的api"><a href="#knn中的api" class="headerlink" title="knn中的api"></a>knn中的api</h3><p><code>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm=&#39;auto&#39;)</code></p><h3 id="KNN中K值大小选择对模型的影响"><a href="#KNN中K值大小选择对模型的影响" class="headerlink" title="KNN中K值大小选择对模型的影响"></a>KNN中K值大小选择对模型的影响</h3><ul><li>K值过小：容易受到异常点的影响，容易过拟合</li><li>K值过大：受到样本均衡的问题，容易欠拟合</li></ul><p>通过以上内容，我们对sklearn工具和KNN算法的API有了初步的了解，并且掌握了如何使用sklearn中的KNN算法进行简单的模型训练和预测。同时，我们也了解了K值选择对模型的影响以及API中其他参数的含义。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="机器学习" scheme="https://www.enju-tsubaki.icu/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="KNN" scheme="https://www.enju-tsubaki.icu/tags/KNN/"/>
    
    <category term="API" scheme="https://www.enju-tsubaki.icu/tags/API/"/>
    
  </entry>
  
  <entry>
    <title>K-近邻算法</title>
    <link href="https://www.enju-tsubaki.icu/posts/2431c9eb.html"/>
    <id>https://www.enju-tsubaki.icu/posts/2431c9eb.html</id>
    <published>2025-02-10T05:26:18.410Z</published>
    <updated>2025-02-14T06:00:04.438Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="🎈1-1-K-近邻算法简介"><a href="#🎈1-1-K-近邻算法简介" class="headerlink" title="🎈1.1 K - 近邻算法简介"></a>🎈1.1 K - 近邻算法简介</h1><h2 id="🌟学习目标"><a href="#🌟学习目标" class="headerlink" title="🌟学习目标"></a>🌟学习目标</h2><ul><li><strong>目标</strong>：了解什么是 KNN 算法</li><li><strong>知道</strong>：KNN 算法求解过程</li></ul><h2 id="🌸1-什么是-K-近邻算法"><a href="#🌸1-什么是-K-近邻算法" class="headerlink" title="🌸1 什么是 K - 近邻算法"></a>🌸1 什么是 K - 近邻算法</h2><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E5%9C%B0%E5%9B%BEK%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.png" alt="Alt text"></p><p>根据你的 “邻居” 来推断出你的类别。</p><h3 id="🐾1-1-K-近邻算法-KNN-概念"><a href="#🐾1-1-K-近邻算法-KNN-概念" class="headerlink" title="🐾1.1 K - 近邻算法 (KNN) 概念"></a>🐾1.1 K - 近邻算法 (KNN) 概念</h3><p>K Nearest Neighbor 算法又叫 KNN 算法，这个算法是机器学习里面一个比较经典的算法，总体来说 KNN 算法是相对比较容易理解的算法。</p><h4 id="💡定义"><a href="#💡定义" class="headerlink" title="💡定义"></a>💡定义</h4><p>如果一个样本在特征空间中的 k 个最相似 (即特征空间中最邻近) 的样本中的大多数属于某一个类别，则该样本也属于这个类别。例如，在一个水果分类问题中，我们有一堆已知类别的水果样本（苹果、橙子等），对于一个未知类别的水果，我们通过计算它与已知水果样本的相似度（距离），找到最相似的 k 个样本，如果这 k 个样本中大多数是苹果，那么我们就可以推断这个未知水果也可能是苹果。</p><h4 id="🌟来源"><a href="#🌟来源" class="headerlink" title="🌟来源"></a>🌟来源</h4><p>KNN 算法最早是由 Cover 和 Hart 提出的一种分类算法。</p><h4 id="📏距离公式"><a href="#📏距离公式" class="headerlink" title="📏距离公式"></a>📏距离公式</h4><p>两个样本的距离可以通过如下公式计算，又叫欧式距离 。<br><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB.png" alt="Alt text"></p><script type="math/tex; mode=display">对于二维平面上点a(x_{1}, y_{1})与b(x{2}, y{2}) 之间的欧氏距离：</script><script type="math/tex; mode=display">d_{12} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}</script><script type="math/tex; mode=display">对于三维空间点 a(x{_1}, y{_1}, z{_1})与b(x{_2}, y{_2}, z{_2}) 之间的欧氏距离：</script><script type="math/tex; mode=display">d_{12} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2}</script><script type="math/tex; mode=display">对于n维空间点 a(x_{11}, x_{12}, \ldots, x_{1n})与b(x_{21}, x_{22}, \ldots, x_{2n}) 之间的欧氏距离（两个n维向量）：</script><script type="math/tex; mode=display">d_{12} = \sqrt{\sum_{k=1}^{n} (x_{1k} - x_{2k})^2}</script><h3 id="🍿1-2-电影类型分析"><a href="#🍿1-2-电影类型分析" class="headerlink" title="🍿1.2 电影类型分析"></a>🍿1.2 电影类型分析</h3><p>假设我们现在有几部电影（电影数据表格，包含电影名称、特征数据、类别等信息）。其中有一部 “？号电影” 不知道类别，如何去预测？我们可以利用 K 近邻算法的思想。</p><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E7%94%B5%E5%BD%B1%E4%B8%BE%E4%BE%8B.png" alt="Alt text"></p><p>分别计算每个电影和被预测电影的距离，然后求解。比如，我们可以从电影的多个特征（如搞笑镜头、拥抱镜头、打斗镜头等）来计算它们之间的距离。</p><p><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E7%94%B5%E5%BD%B1%E4%B8%BE%E4%BE%8B2.png" alt="Alt text"></p><p>假设我们已经计算出了各电影与 “？号电影” 的距离，如下表所示（示例数据）：<br><img src="https://cdn.jsdelivr.net/gh/enju-tsubaki/image/img/knn/%E7%94%B5%E5%BD%B1%E4%B8%BE%E4%BE%8B3.png" alt="Alt text"></p><h3 id="🐼分类过程🐠"><a href="#🐼分类过程🐠" class="headerlink" title="🐼分类过程🐠"></a>🐼分类过程🐠</h3><p>当 (K = 5) 时，从表格中可知距离《唐人街探案》最近的 5 部电影分别是《功夫熊猫》《美人鱼》《宝贝当家》《新步步惊心》《代理情人》。</p><ul><li>这 5 部电影中：</li><li>《功夫熊猫》《美人鱼》《宝贝当家》是喜剧片；</li><li>《新步步惊心》和《代理情人》是爱情片。</li></ul><p>喜剧片的数量为 3，爱情片的数量为 2。  根据 KNN 算法中多数表决的原则，在这 5 个最近邻中，喜剧片的数量占多数。</p><h3 id="🎬结论"><a href="#🎬结论" class="headerlink" title="🎬结论"></a>🎬结论</h3><p>通过 KNN 分析，预测《唐人街探案》的电影类型为喜剧片。因为在距离它最近的 5 部电影中，喜剧片的数量多于其他类型的电影数量。</p><h3 id="📈1-3-KNN-算法流程总结"><a href="#📈1-3-KNN-算法流程总结" class="headerlink" title="📈1.3 KNN 算法流程总结"></a>📈1.3 KNN 算法流程总结</h3><ol><li><strong>计算已知类别数据集中的点与当前点之间的距离</strong>：利用距离公式（如欧氏距离），计算每个已知样本点与待预测点的距离。</li><li><strong>按距离递增次序排序</strong>：将计算得到的距离从小到大进行排序。</li><li><strong>选取与当前点距离最小的 k 个点</strong>：从排序后的距离列表中，选取前 k 个最小距离对应的样本点。</li><li><strong>统计前 k 个点所在的类别出现的频率</strong>：查看这 k 个点分别属于哪些类别，并统计每个类别出现的次数。</li><li><strong>返回前 k 个点出现频率最高的类别作为当前点的预测分类</strong>：如果这 k 个点中属于动作片类别的点最多，那么就预测待预测电影为动作片。</li></ol><h2 id="🌈2-小结"><a href="#🌈2-小结" class="headerlink" title="🌈2 小结"></a>🌈2 小结</h2><ul><li><strong>K - 近邻算法简介【了解】</strong>：就是通过你的 “邻居” 来判断你属于哪个类别。</li><li><strong>如何计算你到你的 “邻居” 的距离</strong>：一般时候，都是使用欧氏距离。欧氏距离能够直观地衡量两个样本在特征空间中的距离远近，帮助我们找到最邻近的样本。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="机器学习" scheme="https://www.enju-tsubaki.icu/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="KNN" scheme="https://www.enju-tsubaki.icu/tags/KNN/"/>
    
  </entry>
  
</feed>
